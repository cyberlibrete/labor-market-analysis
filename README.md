# Анализ рынка труда | Labor Market Insights
Система для комплексного анализа рвнка труда: от сбора данных с открытых платформ вакансий до аналитики, визуализации и подготовки профессиональных отчетов.


## Описание проекта
Labor Market Insights — это модульная аналитическая платформа, предназначенная для систематического исследования рынка труда.

*Проект включает полный цикл обработки данных:*
- автоматический сбор вакансий с HeadHunter, SuperJob;
- очистку, нормализацию и структурирование данных;
- сохранение в локальное хранилище (PostgreSQL/файлы);
- аналитическую обработку с расчетом ключевых показателей рынка;
- визуализацию результатов в виде дашбордов, инфографики и печатных отчетов.

Архитектура проекта ориентированна на воспроизводимость, прозрачность и расширяемость, чтобы можно было легко добавить новые источники и новые виды аналитики.


## Основные цели
- Создать стабильную систему сбора вакансий из нескольких источников.
- Привести все данные к единому формату для удобного анализа.
- Хранить данные локально, обеспечивая возможность повторного чтения.
- Выстроить модуль аналитики рынка труда (зарплаты, навыки, регионы, отрасли).
- Представить современную визуализацию и удобные инструменты интерпретации.

## Архитектура проекта
Проект состоит из шести основных модулей:

### 1. Сбор данных
Задача: автоматическая загрузка вакансий из разных платформ по API.

Функциональность:
- единая базовая модель клиента API;
- конвертация API-ответа в унифицированную структуру;
- логирование и трекинг прогресса.

### 2. Очистка и подготовка данных
Задача: привести разнородные данные к согласованному виду.

Функциональность:
- удаление дубликатов;
- извлечение вложенных структур;
- нормализация зарплатных диапазонов;
- стандартизация полей региона, специализации, компании.

### 3. Сохранение данных
Способ хранения опционален:
- в файлах (`.csv`, `.json`);
- PostgreSQL.

Функциональность:
- единая модель для загрузки данных:
- оптимизированная схема таблиц (вакансии, работодатели, навыки, регионы);
- возможность обновления/дозагрузки данных.

### 4. Чтение данных
Задача: обеспечить повторное использование уже собранных данных.

Функциональность:
- согласованная абстракция загрузки данных;
- фильтрация, агрегирование, выборка по параметрам;
- безопасная работа с подключениями.

### 5. Аналитика рынка труда
Задача: расчет ключевых показателей и построение аналитической модели.

Функциональность:
- средние/меданные зарплаты по регионам, сферам, профессиям;
- динамика публикаций вакансий и спроса на специалистов;
- топ (навыки, работодатели, профессии);
- анализ требований к образованию и опыту;
- оценка конкуренции и востребованности профессий;
- построение агрегированного индекса рынка;
- генерация рейтингов и сравнительных таблиц.

### 6. Визуализация и отчеты
Задача: представить удобный формат представления результатов.

Функциональность:
- интерактивные графики;
- авто-генерация текстовых отчетов;
- экспорт графиков как изображения;
- фианльные аналитические презентации.

## Технологии

Язык: **Python 3.12**
Сторонние библиотеки: **requests, pandas, numpy, scipy, Plotly, Matplotlib, Seaborn SQLAlchemy**


## Архитектура проекта
Проект разделён на четыре ключевых слоя:

1. Data Collection (ETL-сборщик)
- Python-скрипты для работы с API HH, SuperJob и других источников.
- Механизм нормализации данных (приведение к единому формату).

2. Storage Layer (PostgreSQL)
- Локальная база данных.
- Схема таблиц вакансий, работодателей, навыков и статистических показателей.

3. Analytics Layer
- Загрузка данных из PostgreSQL.
- Подсчёт KPI: - популярные навыки;
- уровни зарплат;
- распределение по профессиям;
- динамика публикаций;
- кластеризация вакансий (опционально).

4. Visualization Layer
- Интерактивные графики.
- Возможность создания полноценного дашборда (например, Streamlit / Dash / Jupyter).

## Технологии
- Python 3.11+
- PostgreSQL 15+
- requests / aiohttp для API
- pandas, SQLAlchemy, psycopg2 / asyncpg
- matplotlib / plotly
- Docker (опционально)

## Планы разработки
- Добавить поддержку асинхронной выгрузки вакансий.
- Реализовать систему декоративных метрик (например, индекс «востребованности профессии»).
- Добавить поддержку машинного обучения.
- Построить веб-дашборд.

## Структура файлов проекта
Ниже структура, оптимальная для Python-проекта с ETL + аналитикой:

```bash
labor-market-analytics/
│
├── README.md
├── pyproject.toml                # poetry или setuptools
├── requirements.txt              # зависимости (если без poetry)
├── .env.example                  # пример конфигурации БД / API
├── .gitignore
│
├── data/
│   ├── raw/                      # исходные выгрузки с API
│   ├── cleaned/                  # очищенные и нормализованные данные
│   ├── export/                   # финальный csv/json для анализа/отчета
│   └── cache/                    # временные файлы
│
├── src/
│   ├── collectors/               # сборщики данных
│   │   ├── hh_api.py
│   │   ├── superjob_api.py
│   │   ├── fetch_pipeline.py
│   │   └── base_client.py
│   │
│   ├── cleaning/                 # очистка + нормализация
│   │   ├── normalize.py
│   │   └── mapping.py
│   │
│   ├── db/
│   │   ├── models/               # SQLAlchemy модели
│   │   │   ├── vacancy.py
│   │   │   ├── employer.py
│   │   │   └── skill.py
│   │   │
│   │   ├── schema.sql            # SQL схема
│   │   └── loader.py             # загрузка данных в PostgreSQL
│   │
│   ├── analytics/
│   │   ├── kpi.py                # ключевые показатели
│   │   ├── trends.py             # динамика
│   │   └── skills.py             # анализ навыков
│   │
│   ├── visualization/
│   │   ├── charts.py
│   │   └── dashboard.py
│   │
│   └── utils/
│       ├── config.py
│       ├── logging.py
│       ├── helpers.py
│       └── dir_schema.json
│
├── notebooks/
│   ├── overview.ipynb            # обзор данных
│   └── analytics.ipynb           # аналитические эксперименты
│
└── docs/
    ├── architecture.md
    ├── api_spec.md
    └── db_schema.md
```

## Быстрый старт

```bash
git clone https://github.com/cyberlibrete/labor-market-analysis.git
cd labor-market-analysis
pip install -r requirements.txt
cp .env.example .env
```

1. Укажи API-ключи и данные PostgreSQL

2. Соберите вакансии:
```bash
python src/collectors/fetch_pipeline.py
```

3. Очистить данные:
```bash
python src/cleaning/normalizer.py
```

4. Сохранить данные в PostgreSQL или в файлы.

5. Запустить аналитику:
```bash
python src/analytics/kpi.py
```

6. Создай визуализацию:
```bash
python src/visualization/dashboard.py
```
